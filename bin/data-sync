#!/usr/bin/env python3
"""Synchronizes a live-set’s recordings—stereo WAV pair, multitrack stems, MIDI-event CSV, and camera MP4—into aligned, lossless files:

• Detects a distinctive “one / pause / two / pause / three” burst of six short synth tones in each audio group; if not found, falls back to the first clear onset.
• Trims stereo, multitrack, and video independently at their own cue, writing aligned versions into an output directory that mirrors the input tree and appends “_aligned” before each extension (-c copy keeps video lossless; audio is 24-bit PCM).
• Parses the MIDI log, derives BPM from clock pulses, embeds the tempo in a combined .mid, and writes an extra tempo-mapped .mid for every channel present.
• Embeds the initial BPM tag into the aligned stereo WAVs.

Run the script with paths to the stereo folder, multitrack folder, MIDI CSV, camera MP4, and optional output directory; it returns a fully time-aligned asset set ready for import into a DAW or NLE with no generational quality loss.
"""

from __future__ import annotations

import argparse
import csv
import subprocess
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Tuple

import librosa
import mido
import numpy as np
import soundfile as sf  # noqa: F401

# ---------------------------------------------------------------------------
# FFmpeg helper
# ---------------------------------------------------------------------------

def run_ffmpeg(cmd: List[str]) -> None:
    proc = subprocess.run(cmd, capture_output=True, text=True)
    if proc.returncode != 0:
        raise RuntimeError(proc.stderr)

# ---------------------------------------------------------------------------
# Pulse-pattern detection
# ---------------------------------------------------------------------------

def read_channel(path: Path, sr: int, channel: int) -> np.ndarray:
    print(f"Reading {path}")
    # Read a specific channel from a multi-channel wav file
    data, file_sr = sf.read(path.as_posix(), always_2d=True)
    if file_sr != sr:
        raise RuntimeError(f"Expected sample rate {sr}, got {file_sr}")
    if channel >= data.shape[1]:
        raise RuntimeError(f"Channel {channel} out of range for file {path} (has {data.shape[1]} channels)")
    return data[:, channel]

def detect_123_pattern(audio: np.ndarray, sr: int) -> int | None:
    """Return sample index of the first pulse in a 1-2-3 burst pattern.

    Pattern definition (rough):
      pulse
      ~0.4-1.0 s gap
      pulse   pulse  (≤0.3 s apart)
      ~0.4-1.0 s gap
      pulse   pulse   pulse (≤0.3 s between each)

    If not found, return None.
    """
    # Onset detection – gives frame indices of transient peaks
    oenv = librosa.onset.onset_strength(y=audio, sr=sr)
    peaks = librosa.util.peak_pick(oenv, pre_max=3, post_max=3, pre_avg=3, post_avg=3, delta=0.3, wait=10)
    if len(peaks) < 6:
        return None

    times = peaks * 512 / sr  # onset_strength default hop=512

    # Scan windows of 6 peaks for 1-2-3 pattern
    for i in range(len(times) - 5):
        grp = times[i : i + 6]
        g01 = grp[1] - grp[0]
        g12 = grp[2] - grp[1]
        g23 = grp[3] - grp[2]
        g34 = grp[4] - grp[3]
        g45 = grp[5] - grp[4]

        # first pair: 0.4–1.0s
        in_first_pair = 0.4 <= g01 <= 1.0
        # second pulse pair within ≤0.3s
        in_second_group = g12 <= 0.3
        # gap between the two-pulse and three-pulse group: 0.4–1.0s
        inter_gap = 0.4 <= g23 <= 1.0
        # three-pulse spacing both ≤0.3s
        three_spacing = g34 <= 0.3 and g45 <= 0.3

        if in_first_pair and in_second_group and inter_gap and three_spacing:
            return int(peaks[i] * 512)
    return None

def detect_first_peak(audio: np.ndarray, sr: int) -> int:
    # Onset detection – gives frame indices of transient peaks
    # backtrack=True pins the onset to the nearest preceding maximum
    onset_frames = librosa.onset.onset_detect(y=audio, sr=sr, backtrack=True)
    return int(onset_frames[0] * 512) if len(onset_frames) else 0

def detect_offset(audio: np.ndarray, sr: int) -> int:
    off = detect_123_pattern(audio, sr)
    if off is not None:
        return off
    return detect_first_peak(audio, sr)

# ---------------------------------------------------------------------------
# MIDI utilities
# ---------------------------------------------------------------------------
Event = Tuple[float, str, int, List[int]]

def parse_midi_csv(path: Path) -> List[Event]:
    ev: List[Event] = []
    with path.open() as fh:
        reader = csv.reader(fh)
        for row in reader:
            if not row or row[0].lower() in {"time", "timestamp"}:
                continue

            try:
                t = float(row[0])
                status = row[1]

                if status in {"note_on", "note_off", "cc", "pb"}:
                    ch = int(row[2])
                    if not (0 <= ch <= 15):
                        continue
                    data = [int(x) for x in row[3:] if x.strip()]
                    ev.append((t, status, ch, data))

                elif status == "clock":
                    ev.append((t, status, 0, []))  # valid timing event, no channel

                # skip all other status types

            except (ValueError, IndexError):
                continue

    return ev

def bpm_from_clock(events: List[Event]) -> float:
    clk = [t for t, s, _, _ in events if s == "clock"]
    return float(np.median(60 / (np.diff(clk) * 24))) if len(clk) > 1 else 0.0

def write_midi(events: List[Event], bpm: float, out: Path, ch_filter: int | None = None) -> None:
    mid = mido.MidiFile(); tr = mido.MidiTrack(); mid.tracks.append(tr)
    tempo = int(60_000_000 / bpm) if bpm else 500000
    tr.append(mido.MetaMessage("set_tempo", tempo=tempo, time=0))
    tpq, prev = mid.ticks_per_beat, 0.0
    for t, s, ch, data in events:
        if s == "clock" or (ch_filter is not None and ch != ch_filter):
            continue
        dt = int((t - prev) * bpm * tpq / 60) if bpm else 0; prev = t
        if s == "note_on":
            tr.append(mido.Message("note_on", channel=ch, note=data[0], velocity=data[1], time=dt))
        elif s == "note_off":
            tr.append(mido.Message("note_off", channel=ch, note=data[0], velocity=0, time=dt))
    mid.save(out.as_posix())

# ---------------------------------------------------------------------------
# Path helpers / trim routine
# ---------------------------------------------------------------------------
def ensure_parent(p: Path) -> None: p.parent.mkdir(parents=True, exist_ok=True)

def mirrored(src: Path, base: Path, out_root: Path) -> Path:
    # Output path mirrors input, appending _aligned before extension
    rel = src.relative_to(base); dst = out_root / base.name / rel
    ensure_parent(dst); return dst.with_name(rel.stem + "_aligned" + rel.suffix)

def trim_multichannel_wav(src: Path, off: int, sr: int, base: Path, out_root: Path):
    # Trim multi-channel wav file from sample offset, write to mirrored path
    data, file_sr = sf.read(src.as_posix(), always_2d=True)
    if file_sr != sr:
        raise RuntimeError(f"Expected sample rate {sr}, got {file_sr}")
    start = off
    trimmed = data[start:, :]
    dst = mirrored(src, base, out_root)
    sf.write(dst.as_posix(), trimmed, sr, subtype="PCM_24")
    return dst

def trim_video(files: List[Path], off: int, sr: int, base: Path, out_root: Path):
    # Trim video files using ffmpeg
    start = off / sr
    for f in files:
        dst = mirrored(f, base, out_root)
        cmd = ["ffmpeg", "-y", "-ss", f"{start}", "-i", f.as_posix(), "-c", "copy", dst.as_posix()]
        run_ffmpeg(cmd)

# ---------------------------------------------------------------------------
# Main entry
# ---------------------------------------------------------------------------

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--stereo_wav", type=Path, required=True, help="Path to stereo multichannel WAV")
    ap.add_argument("--multitrack_wav", type=Path, required=True, help="Path to multitrack multichannel WAV")
    ap.add_argument("--midi_log", type=Path, required=True)
    ap.add_argument("--video", type=Path, required=True)
    ap.add_argument("--output_dir", type=Path, default=Path("aligned_output"))
    ap.add_argument("--sr", type=int, default=44100)
    ap.add_argument("--detect_channel", type=int, default=0, help="Channel to use for cue detection")
    args = ap.parse_args(); args.output_dir.mkdir(parents=True, exist_ok=True)

    # ---- collect files ----
    # Instead of collecting multiple mono files, use single multi-channel WAV for stereo and multitrack

    # ---- detect per-group offsets ----
    stereo_off = detect_offset(read_channel(args.stereo_wav, args.sr, args.detect_channel), args.sr)
    multi_off  = detect_offset(read_channel(args.multitrack_wav, args.sr, args.detect_channel), args.sr)

    tmp = args.output_dir / "_vid.wav"
    # run_ffmpeg(["ffmpeg", "-y", "-i", args.video.as_posix(), "-vn", "-acodec", "pcm_s24le", tmp.as_posix()])
    # video_off = detect_offset(read_channel(tmp, args.sr, args.detect_channel), args.sr); tmp.unlink(missing_ok=True)
    run_ffmpeg([
        "ffmpeg", "-y",
        "-i", args.video.as_posix(),
        "-vn",
        "-acodec", "pcm_s24le",  # 24-bit signed little-endian PCM
        "-ar", str(args.sr),     # Explicit sample rate (default: 44100)
        "-ac", "2",              # Force stereo, adjust if needed
        tmp.as_posix()
    ])
    video_off = detect_offset(read_channel(tmp, args.sr, args.detect_channel), args.sr)
    tmp.unlink(missing_ok=True)



    # ---- trim groups independently ----
    trim_multichannel_wav(args.stereo_wav,     stereo_off, args.sr, args.stereo_wav.parent,     args.output_dir)
    trim_multichannel_wav(args.multitrack_wav, multi_off,  args.sr, args.multitrack_wav.parent, args.output_dir)
    trim_video([args.video], video_off, args.sr, args.video.parent, args.output_dir)

    # ---- MIDI handling with offset applied ----
    midi_events = parse_midi_csv(args.midi_log)
    bpm         = bpm_from_clock(midi_events)

    # offset_sec      = stereo_off / args.sr
    # adjusted_events = [(t - offset_sec, status, ch, data) for (t, status, ch, data) in midi_events]
    offset_sec = stereo_off / args.sr

    # Step 1: Apply initial alignment offset
    adjusted = [(t - offset_sec, status, ch, data) for (t, status, ch, data) in midi_events]

    # Step 2: Ensure earliest time is zero by shifting everything forward
    min_t = min((t for t, *_ in adjusted), default=0)
    if min_t < 0:
        adjusted_events = [(t - min_t, status, ch, data) for (t, status, ch, data) in adjusted]
    else:
        adjusted_events = adjusted


    midi_out_path = args.output_dir / "combined_aligned.mid"
    write_midi(adjusted_events, bpm, midi_out_path)

    channels = sorted({ch for _, _, ch, _ in midi_events if ch >= 0})
    for ch in channels:
        ch_path = args.output_dir / f"channel_{ch}_aligned.mid"
        write_midi(adjusted_events, bpm, ch_path, ch_filter=ch)

if __name__ == "__main__":
    main()

