#!/usr/bin/env python3
"""Synchronizes a live-set’s recordings—stereo WAV pair, multitrack stems, MIDI-event CSV, and camera MP4—into aligned, lossless files:

• Detects a distinctive “one / pause / two / pause / three” burst of six short synth tones in each audio group; if not found, falls back to the first clear onset.
• Trims stereo, multitrack, and video independently at their own cue, writing aligned versions into an output directory that mirrors the input tree and appends “_aligned” before each extension (-c copy keeps video lossless; audio is 24-bit PCM).
• Parses the MIDI log, derives BPM from clock pulses, embeds the tempo in a combined .mid, and writes an extra tempo-mapped .mid for every channel present.
• Embeds the initial BPM tag into the aligned stereo WAVs.

Run the script with paths to the stereo folder, multitrack folder, MIDI CSV, camera MP4, and optional output directory; it returns a fully time-aligned asset set ready for import into a DAW or NLE with no generational quality loss.
"""

from __future__ import annotations

import argparse
import csv
import subprocess
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Tuple

import librosa
import mido
import numpy as np
import soundfile as sf  # noqa: F401

# ---------------------------------------------------------------------------
# FFmpeg helper
# ---------------------------------------------------------------------------

def run_ffmpeg(cmd: List[str]) -> None:
    proc = subprocess.run(cmd, capture_output=True, text=True)
    if proc.returncode != 0:
        raise RuntimeError(proc.stderr)

# ---------------------------------------------------------------------------
# Pulse‑pattern detection
# ---------------------------------------------------------------------------

def read_mono(path: Path, sr: int) -> np.ndarray:
    audio, _ = librosa.load(path.as_posix(), sr=sr, mono=True)
    return audio


def detect_123_pattern(audio: np.ndarray, sr: int) -> int | None:
    """Return sample index of the first pulse in a 1‑2‑3 burst pattern.

    Pattern definition (rough):
      pulse
      ~0.4‑1.0 s gap
      pulse   pulse  (≤0.3 s apart)
      ~0.4‑1.0 s gap
      pulse   pulse   pulse (≤0.3 s between each)

    If not found, return None.
    """
    # Onset detection – gives frame indices of transient peaks
    oenv = librosa.onset.onset_strength(y=audio, sr=sr)
    peaks = librosa.util.peak_pick(oenv, pre_max=3, post_max=3, pre_avg=3, post_avg=3, delta=0.3, wait=10)
    if len(peaks) < 6:
        return None

    times = peaks * 512 / sr  # onset_strength default hop=512

    # Scan windows of 6 peaks for 1‑2‑3 pattern
    for i in range(len(times) - 5):
        grp = times[i : i + 6]
        g01 = grp[1] - grp[0]
        g23 = grp[3] - grp[2]
        g45 = grp[5] - grp[4]
        in_pair_1 = grp[2] - grp[1] <= 0.3
        in_pair_2 = grp[4] - grp[3] >= 0.4 and grp[4] - grp[3] <= 1.0  # gap between 2‑click group and 3‑click group starts
        three_spacing = g45 <= 0.3 and grp[4] - grp[3] <= 0.3
        if (
            0.4 <= g01 <= 1.0
            and in_pair_1
            and in_pair_2
            and three_spacing
        ):
            return int(peaks[i] * 512)
    return None


def detect_first_peak(audio: np.ndarray, sr: int) -> int:
    onset_frames = librosa.onset.onset_detect(y=audio, sr=sr, backtrack=False)
    return int(onset_frames[0] * 512) if len(onset_frames) else 0


def detect_offset(audio: np.ndarray, sr: int) -> int:
    off = detect_123_pattern(audio, sr)
    if off is not None:
        return off
    return detect_first_peak(audio, sr)

# ---------------------------------------------------------------------------
# MIDI utilities
# ---------------------------------------------------------------------------
Event = Tuple[float, str, int, List[int]]

def parse_midi_csv(path: Path) -> List[Event]:
    ev: List[Event] = []
    with path.open() as fh:
        for row in csv.reader(fh):
            if not row:
                continue
            t = float(row[0])
            status = row[1]
            if status in {"note_on", "note_off", "cc", "pb"}:
                ch = int(row[2]); data = [int(x) for x in row[3:]]
            else:
                ch, data = -1, [int(x) for x in row[2:]]
            ev.append((t, status, ch, data))
    return ev


def bpm_from_clock(events: List[Event]) -> float:
    clk = [t for t, s, _, _ in events if s == "clock"]
    return float(np.median(60 / (np.diff(clk) * 24))) if len(clk) > 1 else 0.0


def write_midi(events: List[Event], bpm: float, out: Path, ch_filter: int | None = None) -> None:
    mid = mido.MidiFile(); tr = mido.MidiTrack(); mid.tracks.append(tr)
    tempo = int(60_000_000 / bpm) if bpm else 500000
    tr.append(mido.MetaMessage("set_tempo", tempo=tempo, time=0))
    tpq, prev = mid.ticks_per_beat, 0.0
    for t, s, ch, data in events:
        if s == "clock" or (ch_filter is not None and ch != ch_filter):
            continue
        dt = int((t - prev) * bpm * tpq / 60) if bpm else 0; prev = t
        if s == "note_on":
            tr.append(mido.Message("note_on", channel=ch, note=data[0], velocity=data[1], time=dt))
        elif s == "note_off":
            tr.append(mido.Message("note_off", channel=ch, note=data[0], velocity=0, time=dt))
    mid.save(out.as_posix())

# ---------------------------------------------------------------------------
# Path helpers / trim routine
# ---------------------------------------------------------------------------

def ensure_parent(p: Path) -> None: p.parent.mkdir(parents=True, exist_ok=True)

def mirrored(src: Path, base: Path, out_root: Path) -> Path:
    rel = src.relative_to(base); dst = out_root / base.name / rel
    ensure_parent(dst); return dst.with_name(rel.stem + "_aligned" + rel.suffix)


def trim_group(files: List[Path], off: int, sr: int, base: Path, out_root: Path, is_video=False):
    start = off / sr
    for f in files:
        dst = mirrored(f, base, out_root)
        cmd = ["ffmpeg", "-y", "-i", f.as_posix(), "-ss", f"{start}"]
        cmd += (["-c", "copy"] if is_video else ["-c:a", "pcm_s24le"])
        cmd.append(dst.as_posix()); run_ffmpeg(cmd)

# ---------------------------------------------------------------------------
# Main entry
# ---------------------------------------------------------------------------

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--stereo_dir", type=Path, required=True)
    ap.add_argument("--multitrack_dir", type=Path, required=True)
    ap.add_argument("--midi_log", type=Path, required=True)
    ap.add_argument("--video", type=Path, required=True)
    ap.add_argument("--output_dir", type=Path, default=Path("aligned_output"))
    ap.add_argument("--sr", type=int, default=48000)
    args = ap.parse_args(); args.output_dir.mkdir(parents=True, exist_ok=True)

    # ---- collect files ----
    stereo = sorted(args.stereo_dir.glob("*.wav")); multi = sorted(args.multitrack_dir.glob("*.wav"))
    if len(stereo) < 2: raise RuntimeError("Require L & R WAVs in stereo_dir")

    # ---- detect per‑group offsets ----
    stereo_off = detect_offset(read_mono(stereo[0], args.sr), args.sr)
    multi_off = detect_offset(read_mono(multi[-1], args.sr), args.sr) if multi else stereo_off

    tmp = args.output_dir / "_vid.wav"
    run_ffmpeg(["ffmpeg", "-y", "-i", args.video.as_posix(), "-vn", "-acodec", "pcm_s24le", tmp.as_posix()])
    video_off = detect_offset(read_mono(tmp, args.sr), args.sr); tmp.unlink(missing_ok=True)

    # ---- trim groups independently ----
    trim_group(stereo, stereo_off, args.sr, args.stereo_dir, args.output_dir)
    trim_group(multi, multi_off, args.sr, args.multitrack_dir, args.output_dir)
    trim_group([args.video], video_off, args.sr, args.video.parent, args.output

